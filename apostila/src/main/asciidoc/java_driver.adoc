
== Realizando integração com Java

Após toda a teoria, instalação, noção de comunicação com CQL e modelagem finalmente está na hora de juntar tudo isso e colocar dentro de uma aplicação Java. A integração do banco de dados com a linguagem é uma das coisas mais importantes a serem realizadas dentro de uma aplicação e é esse o objetivo dessa capítulo. Ele será dividido com uma comunicação de baixo nível e em seguida se falará de frameworks que abstrai toda essa comunicação além dos seus possíveis impactos na aplicação.

=== Requísitos mínimos para as demonstrações

Para executar todas as demonstrações é necessário que assim como o Cassandra se tenha instalado o Java 8 além do maven superior a versão 3.5.3. No momento que eu escreto o Java se encontra na versão 11, porém, poucos frameworks têm suporte para o uso do mesmo, dessa forma para manter coerência e facilitar o gerenciamento das variáveis de ambiente será utilizado a última versão do Java 8. Como o objetivo do livro é falar sobre o Cassandra será considerado que o leitor tenha noção de Java 8 e maven além de ter os dois instalados e devidamente configurados no sistema operacional favorito além de utilizar qualquer IDE que o leitor se sinta confortável, desde que suporte tanto o uso de maven quanto Java 8.


WARNING: Para seguir lendo as cenas dos próximos capítulos da novela Cassandra é importante que tenha a última atualização do Java 8, maven superior a 3.5.3 e uma IDE que suporte ambos.

Para facilitar a comparação entre as ferramentas de integração com Java será utilizado exatamente o mesmo exemplo. Um outro ponto, com o intuito de diminuir a complexidade e não desviar o foco, os exemplos serão baseados apenas em Java SE, ou seja, executando puramente o bom e velho `public static void main(String[])`. O nosso exemplo será baseado no mesmo caso de uma livraria:

Teremos uma biblioteca do qual terá livros e cada livro terá autor e cada livro tem uma categoria. As consultas desejadas são:

* Buscar o livro a partir do ISBN
* Buscar os livros a partir da respectativa categoria


Pesando de uma maneira simples a modelagem que atenda todas as queries e distribua as informações:

[source,sql]
----
CREATE KEYSPACE IF NOT EXISTS library  WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};
DROP COLUMNFAMILY IF EXISTS library.book;
DROP COLUMNFAMILY IF EXISTS library.category;
DROP TYPE IF EXISTS library.book;

CREATE TYPE IF NOT EXISTS library.book (
    isbn bigint,
    name text,
    author text,
    categories set<text>
);

CREATE COLUMNFAMILY IF NOT EXISTS library.book (
    isbn bigint,
    name text,
    author text,
    categories set<text>,
    PRIMARY KEY (isbn)
);


CREATE COLUMNFAMILY IF NOT EXISTS library.category (
  name text PRIMARY KEY,
  books set<frozen<book>>
);
----

=== O que é DataStax?


Para o próximo passo, iniciaremos a integração com uma framework Java, o nosso primeiro será o driver de comunicação da DataStax. O que ele faz é ter um comportamento muito semelhante ao JDBC no mundo Java, ou seja, esse driver faz a comunicação com baixo nível, isso quer dizer, por exemplo, que isso requer uma grande quantidade de código para realizar a conversão entre uma entidade de negócio e a comunicação com o Cassandra.

O Cassandra é um projeto open source e pertence a Apache fundation, porém, existem diversas empresas que oferecem suporte e serviços ao redor do projeto. A DataStax é uma delas, ela foi fundada pelo Jonathan Ellis umas das pessoas que fez diversas contribuições para o projeto e hoje é Apache Cassandra Chair. A Datastax oferece diversos produtos ao redor do projeto, por exemplo, ferramentas de desenvolvimento, auxiliar o gerenciamento de cluster, drivers de comunicação em diversas linguagens, além de uma solução baseada em Cassandra.

=== Esqueleto dos projetos exemplos

Como já foi mencionado, os exemplos serão baseados em cima do Java SE, assim, uma boa maneira de se criar o projeto além de utilizar a IDE é utilizar o maven archetype como quick-start. Para criar o projeto com esse esqueleto basta executar, por exemplo, o seguinte comando:

[source,bash]
----
mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
----

Um ponto importante é que dentro do pom.xml é importante atualizá-lo para usar com suporte ao Java 8. Assim, o arquivo ficaria da maneira abaixo:

[source,xml]
----
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.mycompany.app</groupId>
    <artifactId>my-app</artifactId>
    <packaging>jar</packaging>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>
        </plugins>
    </build>
    <name>my-app</name>
    <url>http://maven.apache.org</url>
    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>3.8.1</version>
            <scope>test</scope>
        </dependency>
    </dependencies>
</project>
----

TIP: Para cada novo projeto uma boa estratégia seria criar um projeto utilizando o mesmo esqueleto.

=== Utilizando o driver da DataStax

Para começar a desenvolver o código, o primeiro passo é adicionar a dependência do driver, dessa maneira:

[source,xml]
----
<dependency>
    <groupId>com.datastax.cassandra</groupId>
    <artifactId>cassandra-driver-core</artifactId>
    <version>3.6.0</version>
</dependency>
----

TIP: Para facilitar não ativaremos a segurança e estaremos baseado numa instalação local ou com o Docker com o seguinte comando `docker run -d --name casandra-instance -p 9042:9042 cassandra`

Com a dependência dentro do projeto, o próximo passo é iniciar a comunicação. O Cluster é a classe que representa a estrutura de nós do Cassandra, um ponto importante é que com ele é possível utilizar o try-resource, dessa maneira, tão logo o código seja encerrado o cluster é encerrado. A interface Session representa a conexão com o Cassandra, é a partir de uma instância de Session realiza todo o gerenciamento da informação.



.Realiza a primeira comunicação com o Cassandra utilizando o Driver DataStax
[source,java]
----
public class App {

    private static final String KEYSPACE = "library";
    private static final String COLUMN_FAMILY = "book";
    private static final String[] NAMES = new String[]{"isbn", "name", "author", "categories"};

    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();

            Object[] cleanCode = new Object[]{1, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO")};
            Object[] cleanArchitecture = new Object[]{2, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("Good practice")};
            Object[] effectiveJava = new Object[]{3, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "Good practice")};
            Object[] nosql = new Object[]{4, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice")};

            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, cleanCode));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, cleanArchitecture));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, effectiveJava));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, nosql));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, cleanCode));

            ResultSet resultSet = session.execute(QueryBuilder.select().from(KEYSPACE, COLUMN_FAMILY));
            for (Row row : resultSet) {
                Long isbn = row.getLong("isbn");
                String name = row.getString("name");
                String author = row.getString("author");
                Set<String> categories = row.getSet("categories", String.class);
                System.out.println(String.format(" the result is %s %s %s %s", isbn, name, author, categories));
            }
        }

    }

}
----

Na primeira classe demo de interação com o Cassandra, com o Driver, quem já está familiarizado com o Cassandra Query Language se sentirá bastante tranquilo para entender o código. Todas as operações com o CQL são facilitadas a partir da classe `QueryBuilder`, essa classe é uma utilitária que contém diversos métodos que facilitam a vida do desenvolvedor para criar CQL para uma aplicação.

Para o próximo passo, vamos buscar e remover informação dentro da família de coluna `Book` no campo ISBN, que é o partition key. Para facilitar um pouco o código, criaremos um Consumer para logar o resultado (não será nada sofisticado, apenas o bom e velho `System.out.println`).

.Realizando a busca dos campos pelo ID dentro da família da coluna `Book`
[source,java]
----
public class App2 {

    private static final String KEYSPACE = "library";
    private static final String COLUMN_FAMILY = "book";
    private static final String[] NAMES = new String[]{"isbn", "name", "author", "categories"};

    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();

            Object[] cleanCode = new Object[]{1, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO")};
            Object[] cleanArchitecture = new Object[]{2, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("Good practice")};
            Object[] effectiveJava = new Object[]{3, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "Good practice")};
            Object[] nosql = new Object[]{4, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice")};

            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, cleanCode));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, cleanArchitecture));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, effectiveJava));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, nosql));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, cleanCode));

            Consumer<Row> log = row -> {
                Long isbn = row.getLong("isbn");
                String name = row.getString("name");
                String author = row.getString("author");
                Set<String> categories = row.getSet("categories", String.class);
                System.out.println(String.format(" the result is %s %s %s %s", isbn, name, author, categories));
            };

            findById(session,1L, log);

            deleteById(session, 1L);

            PreparedStatement prepare = session.prepare("select * from library.book where isbn = ?");
            BoundStatement statement = prepare.bind(2L);
            ResultSet resultSet = session.execute(statement);
            resultSet.forEach(log);

        }


    }

    private static void deleteById(Session session, Long isbn) {
        session.execute(QueryBuilder.delete().from(KEYSPACE, COLUMN_FAMILY).where(QueryBuilder.eq("isbn", isbn)));

    }

    private static void findById(Session session, long isbn, Consumer<Row> log) {
        ResultSet resultSet = session.execute(QueryBuilder.select().from(KEYSPACE, COLUMN_FAMILY).where(QueryBuilder.eq("isbn", isbn)));
        resultSet.forEach(log);
    }

}
----

WARNING: Para facilitar a legibilidade do código a criação do `Set` foi utilizando o `Sets.newHashSet` que se encontra do Guava uma vez que o Driver do DataStax já o utiliza como dependência.

Na segunda parte do exemplo será operação com a família de coluna `Category` a maior diferença dos exemplos anteriores é que existe o campo UDT para ser tanto inserido como recuperado.


.Manipulando dados dentro da família de coluna `Category`
[source,java]
----
public class App3 {

    private static final String KEYSPACE = "library";
    private static final String TYPE = "book";
    private static final String COLUMN_FAMILY = "category";
    private static final String[] NAMES = new String[]{"name", "books"};

    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();

            UserType userType = session.getCluster().getMetadata().getKeyspace(KEYSPACE).getUserType(TYPE);
            UDTValue cleanCode = getValue(userType, 1, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO", "Good practice", "Design"));
            UDTValue cleanArchitecture = getValue(userType, 2, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("OO", "Good practice"));
            UDTValue effectiveJava = getValue(userType, 3, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "OO", "Good practice"));
            UDTValue nosql = getValue(userType, 4, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice"));

            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"Java", Sets.newHashSet(cleanCode, effectiveJava)}));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"OO", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture)}));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"Good practice", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture, nosql)}));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"NoSQL", Sets.newHashSet(nosql)}));

            ResultSet resultSet = session.execute(QueryBuilder.select().from(KEYSPACE, COLUMN_FAMILY));
            for (Row row : resultSet) {
                String name = row.getString("name");
                Set<UDTValue> books = row.getSet("books", UDTValue.class);
                Set<String> logBooks = new HashSet<>();
                for (UDTValue book : books) {
                    long isbn = book.getLong("isbn");
                    String bookName = book.getString("name");
                    String author = book.getString("author");
                    logBooks.add(String.format(" %d %s %s", isbn, bookName, author));
                }
                System.out.println(String.format("The result %s %s", name, logBooks));

            }
        }

    }

    private static UDTValue getValue(UserType userType, long isbn, String name, String author, Set<String> categories) {
        UDTValue udtValue = userType.newValue();
        TypeCodec<Object> textCodec = CodecRegistry.DEFAULT_INSTANCE.codecFor(DataType.text());
        TypeCodec<Object> setCodec = CodecRegistry.DEFAULT_INSTANCE.codecFor(DataType.set(DataType.text()));
        TypeCodec<Object> bigIntCodec = CodecRegistry.DEFAULT_INSTANCE.codecFor(DataType.bigint());
        udtValue.set("isbn", isbn, bigIntCodec);
        udtValue.set("name", name, textCodec);
        udtValue.set("author", author, textCodec);
        udtValue.set("categories", categories, setCodec);
        return udtValue;

    }

}

----

Para manipulação que envolve os UDT é necessário utilizar uma instância que representa esse tipo o UserType. Tão logo se tenha a instância que representa o tipo book é possível criar valores para esse tipo. Os valores do UDT ficam contidos no UDTValue. Assim como aconteceu na família de coluna `Book` também é possível recuperar/remover pelo ISBN além de percorrer as colunas. De uma maneira geral, é possível realizar toda a operação no CQL e chamá-lo pelo Java com o driver.


.Inserindo, deletando, buscando dentro da família de coluna `Category` é possível perceber também que dentro do exemplo existe uma classe PreparedStatement que possui o mesmo comportamento que existe dentro do JDBC, ou seja, criar uma query do qual é possível trocar as varíaveis em momento de execução.
[source,java]
----
public class App4 {

    private static final String KEYSPACE = "library";
    private static final String TYPE = "book";
    private static final String COLUMN_FAMILY = "category";
    private static final String[] NAMES = new String[]{"name", "books"};

    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();

            UserType userType = session.getCluster().getMetadata().getKeyspace(KEYSPACE).getUserType(TYPE);
            UDTValue cleanCode = getValue(userType, 1, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO", "Good practice", "Design"));
            UDTValue cleanArchitecture = getValue(userType, 2, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("OO", "Good practice"));
            UDTValue effectiveJava = getValue(userType, 3, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "OO", "Good practice"));
            UDTValue nosql = getValue(userType, 4, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice"));

            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"Java", Sets.newHashSet(cleanCode, effectiveJava)}));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"OO", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture)}));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"Good practice", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture, nosql)}));
            session.execute(QueryBuilder.insertInto(KEYSPACE, COLUMN_FAMILY).values(NAMES, new Object[]{"NoSQL", Sets.newHashSet(nosql)}));

            Consumer<Row> log = row -> {
                String name = row.getString("name");
                Set<UDTValue> books = row.getSet("books", UDTValue.class);
                Set<String> logBooks = new HashSet<>();
                for (UDTValue book : books) {
                    long isbn = book.getLong("isbn");
                    String bookName = book.getString("name");
                    String author = book.getString("author");
                    logBooks.add(String.format(" %d %s %s", isbn, bookName, author));
                }
                System.out.println(String.format("The result %s %s", name, logBooks));
            };

            findById(session, "OO", log);
            findById(session, "Good practice", log);
            deleteById(session, "OO");

            PreparedStatement prepare = session.prepare("select * from library.category where name = ?");
            BoundStatement statement = prepare.bind("Java");
            ResultSet resultSet = session.execute(statement);
            resultSet.forEach(log);
        }

    }

    private static void findById(Session session, String name, Consumer<Row> log) {
        ResultSet resultSet = session.execute(QueryBuilder.select().from(KEYSPACE, COLUMN_FAMILY).where(QueryBuilder.eq("name", name)));
        resultSet.forEach(log);
    }

    private static void deleteById(Session session, String name) {
        session.execute(QueryBuilder.delete().from(KEYSPACE, COLUMN_FAMILY).where(QueryBuilder.eq("name", name)));

    }

    private static UDTValue getValue(UserType userType, long isbn, String name, String author, Set<String> categories) {
        UDTValue udtValue = userType.newValue();
        TypeCodec<Object> textCodec = CodecRegistry.DEFAULT_INSTANCE.codecFor(DataType.text());
        TypeCodec<Object> setCodec = CodecRegistry.DEFAULT_INSTANCE.codecFor(DataType.set(DataType.text()));
        TypeCodec<Object> bigIntCodec = CodecRegistry.DEFAULT_INSTANCE.codecFor(DataType.bigint());
        udtValue.set("isbn", isbn, bigIntCodec);
        udtValue.set("name", name, textCodec);
        udtValue.set("author", author, textCodec);
        udtValue.set("categories", categories, setCodec);
        return udtValue;

    }

}
----

=== OxM vs ORM

Após o exemplo utilizando o código driver a pergunta mais frequente que o desenvolvedor realiza é:

Como tornar isso mais simples para as entidades de negócio?

Como programadores temos ciência que apesar do banco de dados trabalhar com estrutura de dados família de coluna é muito comum que o aplicativo como e-commerce, sistema de hospitais, etc. Trabalhem com orientação a objeto. No mundo relacional existe um tipo de framework que cobre essa lacuna que são os **ORM**s (Object-relational mapping) cujo o objetivo é fazer o mapeamento entre o objeto e os bancos relacionais. No mundo dos bancos não relacionais não existe um termo específico para esse tipo de framework, que o sabemos que é o termo ORM não se encaixa uma vez que o *R* se refere aos bancos relacionais. Um conceito que vem cada vez se familirizando é o OxM quem que *x* é qualquer tipo de banco de dados não relacional, ou seja, OxM é um Object-Mapper para qualquer tipo de bancos de dados NoSQL ou simplesmente de mapper.

Esse tipo de ferramenta facilita muito a vida e gera bastante produtividade no mundo de engenharia de software, porém, como qualquer tecnologia pode trazer alguns problemas. O conceito do Object-relational impedance mismatch é desafio encontrado quando se trabalha com mapper dentro de um banco relacional. O fato é que existe uma quebra de paradigma entre o banco relacional e a orientação objetos e como consequência desentendimento entre tais:

Encapsulamento: Um bom design de orientação objeto faz com que os dados sejam bem escondidos, existem diversas citações de boas práticas em livros consagrados, por exemplo, o Clean code que fala que a principal diferença entre estrutura de dados e orientação a objetos é que no segundo expõe o comportamento e esconde os dados. Porém, esse tipo de conceito não existe no relacional.

Herança vs interfaces vs polimorfismo: Apesar de existir diversos bancos de dados que tenham suporte para herança, até o momento que escrevo não existe suporte a recursos como interfaces e polimorfismo. Dentro do aplicativo para escrever um código limpo utilizam recursos como herança e polimorfismos com uma grande frequência.

Com base destes abismos entre paradigmas é muito recorrente que os desenvolvedores em alguns momentos esqueçam que apesar do mapper o banco de dados não é orientado a objetos fazendo com que exista um alto impacto de performance nas aplicações. É muito frequente a referência de que um mapper e um ORM é considerado um anti-pattern e o motivo é simples: é um grande poder que muitos desenvolvedores não utilizam com responsabilidade. Existem maneiras para minimizar esse impacto, dentre eles, começar com a modelagem e para ter uma orientação objeto uma camada entre a estrutura de dados o domínio.

WARNING: Os mappers são poderosas ferramentas para o desenvolvimento, porém, é importante usar esse grande poder de produtividade com contenção.

=== Utilizando o Mapper

Esse mapper também é mantido pela DataStax, ele é uma camada acima da camada do Driver, dessa maneira, quando se fala de dependência é necessário adicionar uma nova dependência além do driver, dando um total de duas dependências.


.Uma grande vantagem do Maven é que não é necessário se preocupar com a dependência da dependência, de uma maneira geral ele fará tudo isso de maneira automática.
[source,xml]
----
<dependency>
    <groupId>com.datastax.cassandra</groupId>
    <artifactId>cassandra-driver-mapping</artifactId>
    <version>3.6.0</version>
</dependency>
----

Para o mesmo exemplo que realiza a manipulação da família de coluna `Book` o primeiro passo é o mapeamento, que nada mais é uma  entidade cujo seus atributos são anotados. Ao ver o código é possível perceber que as anotações são bem similares para quem veio do mundo do JPA, porém, caso o não seja, não sentirá dificuldade uma vez que as anotações são intuitivas. Por exemplo:

* A anotação `Table` é para indicar que a classe é uma
* `Column` indica que o campo será dispersível
* `ParticionKey` indica que aquele atributo desempenha um papel especial dentro da família de coluna que é a de chave primária.


[source,java]
----
@Table(name = "book", keyspace = "library")
public class Book {

    @PartitionKey
    @Column
    private Long isbn;

    @Column
    private String name;

    @Column
    private String author;

    @Column
    private Set<String> categories;

//getter and setter
}
----

No primeiro contato com o mapper, a redução de código é impressionante, o destaque para esse exemplo são as duas novas classes que aparecem: O `MappingManager` e o `Mapper` que servem para gerenciar as instâncias do `Mapper` e facilitar a comunicação entre o CQL e um objeto Java respectivamente.


[source,java]
----
public class App {

    private static final String KEYSPACE = "library";
    private static final String COLUMN_FAMILY = "book";

    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();
            MappingManager manager = new MappingManager(session);
            Mapper<Book> mapper = manager.mapper(Book.class);


            Book cleanCode = getBook(1L, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO"));
            Book cleanArchitecture = getBook(2L, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("Good practice"));
            Book effectiveJava = getBook(3L, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "Good practice"));
            Book nosql = getBook(4L, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice"));

            mapper.save(cleanCode);
            mapper.save(cleanArchitecture);
            mapper.save(effectiveJava);
            mapper.save(nosql);

            Result<Book> books = mapper.map(session.execute(QueryBuilder.select().from(KEYSPACE, COLUMN_FAMILY)));
            for (Book book : books) {
                System.out.println("The result: " + book);
            }
        }

    }

    private static Book getBook(long isbn, String name, String author, Set<String> categories) {
        Book book = new Book();
        book.setIsbn(isbn);
        book.setName(name);
        book.setAuthor(author);
        book.setCategories(categories);
        return book;
    }

}
----

No próximo passo, que é a busca e a remoção de dados pelo ID é possível realizar-lho sem o mínimo contato com o CQL, apenas com as chamadas de métodos dentro do `Mapper`, porém, ele tem uma fina integração com o Driver, por exemplo, para executar CQL e também o `PreparedStatement`.

[source,java]
----
public class App2 {


    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();
            MappingManager manager = new MappingManager(session);
            Mapper<Book> mapper = manager.mapper(Book.class);

            Book cleanCode = getBook(1L, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO"));
            Book cleanArchitecture = getBook(2L, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("Good practice"));
            Book effectiveJava = getBook(3L, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "Good practice"));
            Book nosql = getBook(4L, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice"));

            mapper.save(cleanCode);
            mapper.save(cleanArchitecture);
            mapper.save(effectiveJava);
            mapper.save(nosql);


            Book book = mapper.get(1L);
            System.out.println("Book found: " + book);

            mapper.delete(book);

            System.out.println("Book found: " + mapper.get(1L));

            PreparedStatement prepare = session.prepare("select * from library.book where isbn = ?");
            BoundStatement statement = prepare.bind(2L);
            Result<Book> books = mapper.map(session.execute(statement));
            StreamSupport.stream(books.spliterator(), false).forEach(System.out::println);
        }

    }

    private static Book getBook(long isbn, String name, String author, Set<String> categories) {
        Book book = new Book();
        book.setIsbn(isbn);
        book.setName(name);
        book.setAuthor(author);
        book.setCategories(categories);
        return book;
    }

}
----


Seguindo a mesma linha do exemplo anterior, com driver de comunicação, será criado o novo mapeamento, agora para o `Category`. As novas anotações são o `UDT` que define a classe `BookType` como uma classe do tipo UDT, o `Field` que realiza o mesmo papel do `Column`, porém, dentro de uma classe do tipo `UDT` além do `Frozen` que indica que o não é possível realizar atualização de apenas um campo ou um elemento desse campo.

[source,java]
----
@Table(name = "category", keyspace = "library")
public class Category {

    @PartitionKey
    @Column
    private String name;

    @Frozen
    private Set<BookType> books;
 //getter and setter
}

@UDT(name = "book", keyspace = "library")
public class BookType {

    @Field
    private Long isbn;

    @Field
    private String name;

    @Field
    private String author;

    @Field
    private Set<String> categories;

//getter and setter

}
----

Com a modelagem pronta, o código se torna bem semelhante a interação do `Book` com exceção da criação do Mapper que agora usará o `Category.class` como parâmetro e, obviamente, serão criados instâncias do tipo `Category` para a persistência da informação.

[source,java]
----
public class App3 {

    private static final String KEYSPACE = "library";
    private static final String COLUMN_FAMILY = "category";

    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();
            MappingManager manager = new MappingManager(session);
            Mapper<Category> mapper = manager.mapper(Category.class);

            BookType cleanCode = getBook(1L, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO"));
            BookType cleanArchitecture = getBook(2L, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("Good practice"));
            BookType effectiveJava = getBook(3L, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "Good practice"));
            BookType nosqlDistilled = getBook(4L, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice"));


            Category java = getCategory("Java", Sets.newHashSet(cleanCode, effectiveJava));
            Category oo = getCategory("OO", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture));
            Category goodPractice = getCategory("Good practice", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture, nosqlDistilled));
            Category nosql = getCategory("NoSQL", Sets.newHashSet(nosqlDistilled));

            mapper.save(java);
            mapper.save(oo);
            mapper.save(goodPractice);
            mapper.save(nosql);

            ResultSet resultSet = session.execute(QueryBuilder.select().from(KEYSPACE, COLUMN_FAMILY));
            Result<Category> categories = mapper.map(resultSet);
            StreamSupport.stream(categories.spliterator(), false).forEach(System.out::println);
        }

    }

    private static Category getCategory(String name, Set<BookType> books) {
        Category category = new Category();
        category.setName(name);
        category.setBooks(books);
        return category;
    }

    private static BookType getBook(long isbn, String name, String author, Set<String> categories) {
        BookType book = new BookType();
        book.setIsbn(isbn);
        book.setName(name);
        book.setAuthor(author);
        book.setCategories(categories);
        return book;
    }

}
----

Para a busca pelo nome da categoria o código se mantém bastante estável, isso deixa claro realmente o poder e a produtividade do mapeamento. Com uma instância  criada, a manipulação acontece de maneira bastante fluída.

[source,java]
----
public class App4 {


    public static void main(String[] args) {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();
            MappingManager manager = new MappingManager(session);
            Mapper<Category> mapper = manager.mapper(Category.class);

            BookType cleanCode = getBook(1L, "Clean Code", "Robert Cecil Martin", Sets.newHashSet("Java", "OO"));
            BookType cleanArchitecture = getBook(2L, "Clean Architecture", "Robert Cecil Martin", Sets.newHashSet("Good practice"));
            BookType effectiveJava = getBook(3L, "Effective Java", "Joshua Bloch", Sets.newHashSet("Java", "Good practice"));
            BookType nosqlDistilled = getBook(4L, "Nosql Distilled", "Martin Fowler", Sets.newHashSet("NoSQL", "Good practice"));


            Category java = getCategory("Java", Sets.newHashSet(cleanCode, effectiveJava));
            Category oo = getCategory("OO", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture));
            Category goodPractice = getCategory("Good practice", Sets.newHashSet(cleanCode, effectiveJava, cleanArchitecture, nosqlDistilled));
            Category nosql = getCategory("NoSQL", Sets.newHashSet(nosqlDistilled));

            mapper.save(java);
            mapper.save(oo);
            mapper.save(goodPractice);
            mapper.save(nosql);

            Category category = mapper.get("Java");
            System.out.println(category);
            mapper.delete("Java");

            PreparedStatement prepare = session.prepare("select * from library.category where name = ?");
            BoundStatement statement = prepare.bind("Java");
            Result<Category> resultSet = mapper.map(session.execute(statement));
            StreamSupport.stream(resultSet.spliterator(), false).forEach(System.out::println);
        }

    }

    private static Category getCategory(String name, Set<BookType> books) {
        Category category = new Category();
        category.setName(name);
        category.setBooks(books);
        return category;
    }

    private static BookType getBook(long isbn, String name, String author, Set<String> categories) {
        BookType book = new BookType();
        book.setIsbn(isbn);
        book.setName(name);
        book.setAuthor(author);
        book.setCategories(categories);
        return book;
    }

}
----

WARNING: No mapper existem métodos com o sufixo `Async` que realizará operações de maneira assíncrona para o desenvolvedor. O livro não cobrirá todos os recursos do Mapper, para mais informações consulte a documentação:
https://docs.datastax.com/en/developer/java-driver/3.6/manual/object_mapper/

Impressionado com o poder do Mapper? Isso não é tudo, também é possível criar interfaces assessoras que são interfaces que tem o objetivo de ler e escrever a partir do Cassandra, para isso, é necessário escrever criar interfaces que tenham métodos com a anotação `Query` essa anotação terá o CQL que será executado quando o método for chamado, vale salientar, que é possível ter parâmetros dentro desses métodos como será exibido com uma interface que realizará operações dentro do Book. A cereja do bolo é definir essa interface como assessoras, para isso, é necessário adicionar a anotação `Accessor` nela.


[source,java]
----
@Accessor
public interface BookAccessor {

    @Query("SELECT * FROM library.book")
    Result<Book> getAll();


    @Query("SELECT * FROM library.book where isbn = ?")
    Book findById(long isbn);

    @Query("SELECT * FROM library.book where isbn = :isbn")
    Book findById2(@Param("isbn") long isbn);

}
----

Pronto, interface criada! Agora é apenas utilizar o `MappingManager` para implementá-la e “chutar para o gol”.

[source,java]
----
public class App6 {

    public static void main(String[] args) throws Exception {
        try (Cluster cluster = Cluster.builder().addContactPoint("127.0.0.1").build()) {

            Session session = cluster.connect();
            MappingManager manager = new MappingManager(session);
            BookAccessor bookAccessor = manager.createAccessor(BookAccessor.class);

            Result<Book> all = bookAccessor.getAll();
            StreamSupport.stream(all.spliterator(), false).forEach(System.out::println);

            Book book = bookAccessor.findById(1L);
            Book book2 = bookAccessor.findById(2L);
            System.out.println(book);
            System.out.println(book2);
        }

    }

}
----

Nesse capítulo se obteve o marco da integração entre o Cassandra e um aplicativo Java. Também foi possível ver a diferença de um framework que realiza a comunicação de baixo nível, semelhante ao JDBC, e um mapper do qual ao mesmo tempo que facilita o desenvolvimento aumenta a responsabilidade para que o desenvolvedor não cometa erros por esquecer que existe uma quebra de paradigma entre a aplicação e o banco de dados.
